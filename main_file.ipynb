{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e991f180",
   "metadata": {},
   "source": [
    "# Loading and Cleaning data in chunks\n",
    "To be able to work with all the files, they must first be cleaned and saved as parquet files\n",
    "These files will be saved in chunks of 100,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b271b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import dask.dataframe as dd\n",
    "chunksize = 400_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06135ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths for large csv files\n",
    "\n",
    "file_paths = [\n",
    "    \"./data/raw/2019/cargodesc_files/ams__cargodesc_2019__202001080000_part_0.csv\", \n",
    "    \"./data/raw/2019/cargodesc_files/ams__cargodesc_2019__202001080000_part_1.csv\", \n",
    "    \"./data/raw/2019/cargodesc_files/ams__cargodesc_2019__202001080000_part_2.csv\",\n",
    "    \"./data/raw/2019/cargodesc_files/ams__cargodesc_2019__202001080000_part_3.csv\",\n",
    "    \"./data/raw/2019/cargodesc_files/ams__cargodesc_2019__202001080000_part_4.csv\",\n",
    "    \"./data/raw/2019/container_files/ams__container_2019__202001080000_part_0.csv\",\n",
    "    \"./data/raw/2019/container_files/ams__container_2019__202001080000_part_1.csv\",\n",
    "    \"./data/raw/2019/container_files/ams__container_2019__202001080000_part_2.csv\",\n",
    "    \"./data/raw/2019/container_files/ams__container_2019__202001080000_part_3.csv\",\n",
    "    \"./data/raw/2019/header_files/ams__header_2019__202001080000_part_0.csv\",\n",
    "    \"./data/raw/2019/header_files/ams__header_2019__202001080000_part_1.csv\",\n",
    "    \"./data/raw/2019/header_files/ams__header_2019__202001080000_part_2.csv\",#11\n",
    "    \"./data/raw/2019/header_files/ams__header_2019__202001080000_part_3.csv\", #12\n",
    "    \"./data/raw/2020/cargodesc_files/ams__cargodesc_2020__202009291500_part_0.csv\",#13\n",
    "    \"./data/raw/2020/cargodesc_files/ams__cargodesc_2020__202009291500_part_1.csv\",#14\n",
    "    \"./data/raw/2020/cargodesc_files/ams__cargodesc_2020__202009291500_part_2.csv\",#15\n",
    "    \"./data/raw/2020/cargodesc_files/ams__cargodesc_2020__202009291500_part_3.csv\",#16\n",
    "    \"./data/raw/2020/container_files/ams__container_2020__202009291500_part_0.csv\",#17\n",
    "    \"./data/raw/2020/container_files/ams__container_2020__202009291500_part_1.csv\",#18\n",
    "    \"./data/raw/2020/container_files/ams__container_2020__202009291500_part_2.csv\",#19\n",
    "    \"./data/raw/2020/header_files/ams__header_2020__202009291500_part_0.csv\",#20\n",
    "    \"./data/raw/2020/header_files/ams__header_2020__202009291500_part_1.csv\",#21\n",
    "    \"./data/raw/2020/header_files/ams__header_2020__202009291500_part_2.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec950984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo desc files\n",
    "i = 0\n",
    "for chunk in pd.read_csv(file_paths[-1],chunksize=chunksize):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.drop(['description_sequence_number','piece_count'],axis=1,inplace=True)\n",
    "    chunk['description_text'] = (\n",
    "        chunk['description_text'].astype(str)\n",
    "        .str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', ' ', regex=True)\n",
    "        .str.replace(r'\\.\\s*\\.', '.',regex=True)\n",
    "        .str.replace(r'[\",]+','',regex=True)\n",
    "        .str.replace(r'\\s*\\.\\s*$','',regex=True)\n",
    "    )\n",
    "    chunk.to_parquet(f\"./data/cleaned/2020/cargodesc_files/chunk_{i}.parquet\",index=False)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef840af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container files\n",
    "i = 0\n",
    "for chunk in pd.read_csv(file_paths[-1],chunksize=chunksize):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.drop(['container_type',\n",
    "                'load_status',\n",
    "                'seal_number_1',\n",
    "                'seal_number_2',\n",
    "                'equipment_description_code',\n",
    "                'container_type'\n",
    "                ],axis=1,inplace=True)\n",
    "    chunk = chunk.loc[~((chunk['container_length'] == 0) & (chunk['container_width'] == 0) & (chunk['container_height'] == 0))]\n",
    "    chunk.dropna(thresh=5,inplace=True)\n",
    "    chunk.to_parquet(f\"./data/cleaned/2020/container_files/chunk_{i}.parquet\",index=False)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c208a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable code that will be used to read the csv files as chunks, clean the data and then save them as a parquet file for space reasons\n",
    "\n",
    "i = 0\n",
    "for chunk in pd.read_csv(file_paths[22],chunksize=chunksize):\n",
    "    chunk.drop_duplicates(inplace=True)\n",
    "    chunk.drop([\n",
    "    'carrier_code',\n",
    "    'vessel_country_code',\n",
    "    'foreign_port_of_lading_qualifier',\n",
    "    'record_status_indicator',\n",
    "    'foreign_port_of_destination_qualifier',\n",
    "    'place_of_receipt',\n",
    "    'conveyance_id_qualifier',\n",
    "    'conveyance_id',\n",
    "    'in_bond_entry_type',\n",
    "    'secondary_notify_party_1',\n",
    "    'secondary_notify_party_2',\n",
    "    'secondary_notify_party_3',\n",
    "    'secondary_notify_party_4',\n",
    "    'secondary_notify_party_5',\n",
    "    'secondary_notify_party_6',\n",
    "    'secondary_notify_party_7',                                                                     \n",
    "    'secondary_notify_party_8',\n",
    "    'secondary_notify_party_9',\n",
    "    'secondary_notify_party_10',\n",
    "    ],axis=1,inplace=True)\n",
    "    chunk.dropna(thresh=6,inplace=True)\n",
    "\n",
    "    chunk = chunk[chunk['weight'] != 0]\n",
    "    chunk.to_parquet(f\"data/cleaned/2020/header_files/chunk_{i}.parquet\",index=False)\n",
    "    i +=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ce81c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the file has been chunked and there's now a bunch of files\n",
    "# We can append all the files into one single file for clarity\n",
    "files = glob(\"data/cleaned/2020/header_files/chunk_*.parquet\")\n",
    "df = pd.concat([pd.read_parquet(f) for f in files], ignore_index=True)\n",
    "\n",
    "df.to_parquet(\"data/cleaned/2020/header_files/ams_header_2020_part_2.parquet\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da55a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is to merge all the dfs into one for easier merging with the header csv\n",
    "dfs_cargodesc_list = [\n",
    "]\n",
    "for file in cleaned_fps_2019_cargodesc:\n",
    "    df = pd.read_parquet(file)\n",
    "    dfs_cargodesc_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d29c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cargodesc_combined = pd.concat(dfs_cargodesc_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9364dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = []\n",
    "for df in dfs_header_list:\n",
    "    merge = df.merge(\n",
    "        df_cargodesc_combined[['identifier','description_text']],\n",
    "        on='identifier',\n",
    "        how='left'\n",
    "    )\n",
    "    merged.append(merge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
